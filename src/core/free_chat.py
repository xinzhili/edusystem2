"""
Enterprise NL2SQL Demo for PostgreSQL
=====================================

A refactored demonstration of a Text-to-SQL pipeline for PostgreSQL, 
showcasing complex educational data analysis scenarios.

Key Features:
1. PostgreSQL backend with pgvector extension
2. Educational data schema (students, study details, learning summaries)
3. Multi-table join analysis capabilities
4. Vector retrieval for schema matching

Author: Leo (Adapted by Gemini)
License: MIT
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import psycopg2
from psycopg2 import sql
from psycopg2.extras import DictCursor

# Conditional imports for LLM providers
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# --- Configuration Block ----------------------------------------------------
CONFIG = {
    "database": {
        "dbname": "learning_db",
        "user": "postgres",
        "password": "123456",
        "host": "localhost",
        "port": "5433"
    },
    "embedding_model": "text-embedding-v4",
    "llm": {
        "provider": "dashscope",  # or "openai"
        "api_key_env": "DASHSCOPE_API_KEY",
        "models": {
            "sql_generation": "qwen-plus",
            "answer_generation": "qwen-plus",
        }
    },
    "prompts": {
        "sql_generation": """
ä½ æ˜¯ä¸€ä½PostgreSQLæ•°æ®åº“ä¸“å®¶ã€‚æ ¹æ®ç»™å®šçš„æ•°æ®åº“æ¨¡å¼å’Œè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚

**é‡è¦çº¦æŸ**ï¼š
1. åªèƒ½ä½¿ç”¨æä¾›çš„æ•°æ®åº“æ¨¡å¼ä¸­æ˜ç¡®å­˜åœ¨çš„è¡¨å’Œå­—æ®µ
2. å¦‚æœé—®é¢˜è¦æ±‚çš„æ•°æ®åœ¨ç»™å®šçš„DDLä¸­ä¸å­˜åœ¨ï¼Œå¿…é¡»æ‹’ç»ç”ŸæˆSQL
3. æ‹’ç»æ—¶è¿”å›ï¼šSCHEMA_INSUFFICIENT: [å…·ä½“è¯´æ˜ç¼ºå°‘ä»€ä¹ˆæ•°æ®]
4. PostgreSQLç‰¹æœ‰è¯­æ³•ï¼šä½¿ç”¨ILIKEä»£æ›¿LIKEè¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…

### æ•°æ®åº“æ¨¡å¼:
{schema_context}

### é—®é¢˜:
{question}

### è¦æ±‚:
- å¦‚æœæ‰€éœ€å­—æ®µéƒ½å­˜åœ¨ï¼šè¿”å›çº¯SQLè¯­å¥ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–markdownæ ¼å¼
- å¦‚æœç¼ºå°‘å¿…è¦å­—æ®µï¼šè¿”å› SCHEMA_INSUFFICIENT: [è¯´æ˜åŸå› ]

SQL:
""",
        "answer_generation": """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™å­¦æ•°æ®åˆ†æå¸ˆã€‚åŸºäºç”¨æˆ·çš„é—®é¢˜ã€SQLæŸ¥è¯¢å’Œæ•°æ®ç»“æ„æ‘˜è¦ï¼Œæä¾›æœ‰ä»·å€¼çš„åˆ†æå›ç­”ã€‚

æ³¨æ„ï¼šå‡ºäºæ•°æ®å®‰å…¨è€ƒè™‘ï¼Œä½ æ”¶åˆ°çš„æ˜¯æ•°æ®ç»“æ„æ‘˜è¦è€Œéå®é™…æ•°æ®å€¼ã€‚è¯·åŸºäºæŸ¥è¯¢é€»è¾‘å’Œæ•°æ®ç»“æ„æä¾›ä¸“ä¸šåˆ†æã€‚

### ç”¨æˆ·é—®é¢˜:
{question}

### æ‰§è¡Œçš„SQLæŸ¥è¯¢:
{sql_query}

### æ•°æ®ç»“æ„æ‘˜è¦:
{data_summary}

### åˆ†æè¦æ±‚:
1. åŸºäºSQLæŸ¥è¯¢é€»è¾‘åˆ†ææ•™å­¦é—®é¢˜
2. è§£é‡ŠæŸ¥è¯¢æ¶‰åŠçš„æ•™å­¦æŒ‡æ ‡å’Œå…³ç³»
3. æ ¹æ®æ•°æ®ç»“æ„æä¾›åˆç†çš„æ•™å­¦æ´å¯Ÿ
4. æä¾›æ•°æ®é©±åŠ¨çš„æ•™å­¦å»ºè®®ï¼ˆå¦‚é€‚ç”¨ï¼‰

### ä¸“ä¸šåˆ†æ:
"""
    }
}

# --- Logging Setup ----------------------------------------------------------
log_format = '%(asctime)s - %(levelname)s - [%(name)s] %(message)s'

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

if logger.handlers:
    logger.handlers.clear()

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter(log_format)
console_handler.setFormatter(console_formatter)

file_handler = logging.FileHandler('pg_nl2sql_demo.log', mode='w', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter(log_format)
file_handler.setFormatter(file_formatter)

logger.addHandler(console_handler)
logger.addHandler(file_handler)

# --- Data Classes -----------------------------------------------------------
@dataclass
class TableSchema:
    """Represents a database table schema with metadata."""
    name: str
    ddl: str
    description: str
    
@dataclass
class QueryResult:
    """Represents the result of a SQL query execution."""
    success: bool
    data: List[Dict[str, Any]]
    sql: str
    error: Optional[str] = None

# --- PostgreSQL Components --------------------------------------------------

class PGManager:
    """Manages all PostgreSQL database interactions."""
    def __init__(self, db_config: Dict[str, Any]):  
        self.db_config = db_config
        logger.info(f"PGManager initialized for database: {db_config['dbname']}")
        self._init_database()
    
    def _init_database(self):
        """Initializes the database with educational schema if not present."""
        logger.info("Initializing PostgreSQL database schema...")
        
        try:
            with self._get_connection() as conn:
                with conn.cursor() as cursor:
                    # Check if tables already exist
                    cursor.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'students'
                    """)
                    if cursor.fetchone():
                        logger.info("Database schema already exists. Skipping creation.")
                        return

                    logger.info("Creating educational schema...")
                    self._create_educational_schema(cursor)
                    self._insert_sample_data(cursor)
                    
                    conn.commit()
                    logger.info("Database initialized successfully.")
        except Exception as e:
            logger.error(f"Database initialization failed: {e}", exc_info=True)
            raise

    def _get_connection(self):
        """Returns a new database connection."""
        return psycopg2.connect(**self.db_config)

    def _create_educational_schema(self, cursor):
        """Creates the educational database schema."""
        # 1. å­¦ç”Ÿè¡¨
        cursor.execute("""
        CREATE TABLE students (
            student_id SERIAL PRIMARY KEY,   --å­¦ç”ŸID
            name VARCHAR(20) NOT NULL,       --å­¦ç”Ÿå§“å
            grade SMALLINT NOT NULL,         --å¹´çº§
            date_of_birth DATE NOT NULL,    --å‡ºç”Ÿæ—¥æœŸ
            gender TEXT NOT NULL,           --æ€§åˆ«
            region VARCHAR(20) NOT NULL,    --åœ°åŒºï¼ˆå¦‚ï¼šåä¸œã€ååŒ—ã€åå—ç­‰ï¼‰
            textbook_version VARCHAR(20) NOT NULL,  --æ•™æç‰ˆæœ¬ï¼ˆå¦‚ï¼šäººæ•™ç‰ˆã€åŒ—å¸ˆå¤§ç‰ˆã€è‹æ•™ç‰ˆç­‰ï¼‰
            school VARCHAR(30) NOT NULL,    --å­¦æ ¡
            photo BYTEA,   --å­¦ç”Ÿç…§ç‰‡ï¼Œå­˜å‚¨ä¸ºäºŒè¿›åˆ¶æ•°æ®
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP  --    åˆ›å»ºæ—¶é—´
        );
        """)
        
        # 2. åŸå§‹è¾“å…¥è¡¨
        cursor.execute("""
        CREATE TABLE original_input (
            original_input_id SERIAL PRIMARY KEY,    --åŸå§‹è¾“å…¥ID
            student_id INT REFERENCES students(student_id) ON DELETE SET NULL,  --å…³è”å­¦ç”ŸID
            content BYTEA,           --åŸå§‹è¾“å…¥å†…å®¹ï¼Œå­˜å‚¨ä¸ºäºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚å›¾ç‰‡ã€æ–‡æ¡£ç­‰ï¼‰
            content_hash VARCHAR(32) UNIQUE NOT NULL,    --å†…å®¹çš„MD5å“ˆå¸Œå€¼
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP   --åˆ›å»ºæ—¶é—´
        );
        """)
        
        # 3. å­¦ç”Ÿå­¦ä¹ é”™é¢˜è¡¨
        cursor.execute("""
        CREATE TABLE study_detail (
            study_detail_id SERIAL PRIMARY KEY,   --å­¦ä¹ æ˜ç»†ID
            student_id INT REFERENCES students(student_id) ON DELETE CASCADE,   --å…³è”å­¦ç”ŸID
            original_input_id INT REFERENCES original_input(original_input_id) ON DELETE SET NULL,  --å…³è”åŸå§‹è¾“å…¥ID
            details JSONB NOT NULL,   --é”™é¢˜è¯¦æƒ…ï¼Œå­˜å‚¨ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«é”™é¢˜ã€çŸ¥è¯†ç‚¹ç­‰ä¿¡æ¯
            details_embedding vector(1024),  --é”™é¢˜è¯¦æƒ…çš„å‘é‡åŒ–è¡¨ç¤ºï¼Œä½¿ç”¨pgvectorå­˜å‚¨
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
        );
        """)
        
        # 4. å­¦æƒ…æ±‡æ€»è¡¨
        cursor.execute("""
        CREATE TABLE summary (
            summary_id SERIAL PRIMARY KEY,   --å­¦æƒ…æ±‡æ€»ID
            student_id INT REFERENCES students(student_id) ON DELETE CASCADE, --    å…³è”å­¦ç”ŸID
            grade SMALLINT NOT NULL,         --å¹´çº§
            from_date DATE NOT NULL,         --æ±‡æ€»å¼€å§‹æ—¥æœŸ
            to_date DATE NOT NULL,           --æ±‡æ€»ç»“æŸæ—¥æœŸ
            subject TEXT NOT NULL,           --å­¦ç§‘ï¼ˆå¦‚ï¼šæ•°å­¦ã€è¯­æ–‡ã€è‹±è¯­ç­‰ï¼‰
            details JSONB NOT NULL,          --æ±‡æ€»è¯¦æƒ…ï¼Œå­˜å‚¨ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«å­¦ç§‘ä¼˜åŠ¿ã€è–„å¼±ç¯èŠ‚ç­‰ä¿¡æ¯
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
        );
        """)
        
        logger.info("4 educational tables created successfully.")

    def _insert_sample_data(self, cursor):
        """Inserts comprehensive sample data for educational analysis."""
        # 1. æ’å…¥å­¦ç”Ÿæ•°æ®
        cursor.executemany("""
        INSERT INTO students (name, grade, date_of_birth, gender, region, textbook_version, school)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """, [
            ('å¼ ä¸‰', 7, '2010-05-15', 'ç”·', 'åä¸œ', 'äººæ•™ç‰ˆ', 'ç¬¬ä¸€ä¸­å­¦'),
            ('æå››', 8, '2009-08-22', 'ç”·', 'ååŒ—', 'åŒ—å¸ˆå¤§ç‰ˆ', 'å®éªŒä¸­å­¦'),
            ('ç‹äº”', 7, '2010-03-10', 'ç”·', 'åå—', 'è‹æ•™ç‰ˆ', 'å¤–å›½è¯­å­¦æ ¡'),
            ('èµµå…­', 8, '2009-11-18', 'å¥³', 'åä¸œ', 'äººæ•™ç‰ˆ', 'ç¬¬ä¸€ä¸­å­¦'),
            ('é’±ä¸ƒ', 9, '2008-07-05', 'å¥³', 'ååŒ—', 'åŒ—å¸ˆå¤§ç‰ˆ', 'å®éªŒä¸­å­¦'),
            ('å­™å…«', 7, '2010-01-30', 'ç”·', 'åå—', 'è‹æ•™ç‰ˆ', 'å¤–å›½è¯­å­¦æ ¡')
        ])
        
        # 2. æ’å…¥åŸå§‹è¾“å…¥æ•°æ®
        cursor.executemany("""
        INSERT INTO original_input (student_id, content_hash)
        VALUES (%s, %s)
        """, [
            (1, 'hash1'),
            (2, 'hash2'),
            (3, 'hash3'),
            (4, 'hash4'),
            (5, 'hash5'),
            (6, 'hash6')
        ])
        
        # 3. æ’å…¥å­¦ç”Ÿå­¦ä¹ é”™é¢˜è¡¨
        study_details = [
            {
                "student_id": 1,
                "original_input_id": 1,
                "details": {
                    "student_question": "è§£æ–¹ç¨‹: 2x + 5 = 15",
                    "student_answer": "x = 10",
                    "correct_answer": "x = 5",
                    "error_type": "è®¡ç®—é”™è¯¯",
                    "analysis": "å¿˜è®°åœ¨ç­‰å¼ä¸¤è¾¹åŒæ—¶å‡5",
                    "subject": "æ•°å­¦",
                    "knowledge_grade": "ä¸ƒå¹´çº§",
                    "knowledge_points": ["ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹", "ç­‰å¼æ€§è´¨"],
                    "difficulty": 2,
                    "true_false_flag": False
                }
            },
            {
                "student_id": 1,
                "original_input_id": 1,
                "details": {
                    "student_question": "è®¡ç®—åœ†çš„é¢ç§¯ï¼ŒåŠå¾„r=3cm",
                    "student_answer": "18.84",
                    "correct_answer": "28.26",
                    "error_type": "å…¬å¼é”™è¯¯",
                    "analysis": "ä½¿ç”¨äº†å‘¨é•¿å…¬å¼2Ï€rè€Œéé¢ç§¯å…¬å¼Ï€rÂ²",
                    "subject": "æ•°å­¦",
                    "knowledge_grade": "ä¸ƒå¹´çº§",
                    "knowledge_points": ["åœ†çš„é¢ç§¯"],
                    "difficulty": 3,
                    "true_false_flag": False
                }
            },
            {
                "student_id": 2,
                "original_input_id": 2,
                "details": {
                    "student_question": "å…‰åˆä½œç”¨çš„äº§ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "student_answer": "æ°§æ°”",
                    "correct_answer": "æ°§æ°”å’Œæœ‰æœºç‰©",
                    "error_type": "çŸ¥è¯†ä¸å®Œæ•´",
                    "analysis": "åªå›ç­”äº†éƒ¨åˆ†äº§ç‰©",
                    "subject": "ç”Ÿç‰©",
                    "knowledge_grade": "å…«å¹´çº§",
                    "knowledge_points": ["å…‰åˆä½œç”¨"],
                    "difficulty": 3,
                    "true_false_flag": False
                }
            },
            {
                "student_id": 3,
                "original_input_id": 3,
                "details": {
                    "student_question": "ã€Šçº¢æ¥¼æ¢¦ã€‹çš„ä½œè€…æ˜¯è°ï¼Ÿ",
                    "student_answer": "æ›¹é›ªèŠ¹",
                    "correct_answer": "æ›¹é›ªèŠ¹",
                    "error_type": None,
                    "analysis": "å›ç­”æ­£ç¡®",
                    "subject": "è¯­æ–‡",
                    "knowledge_grade": "ä¸ƒå¹´çº§",
                    "knowledge_points": ["æ–‡å­¦å¸¸è¯†"],
                    "difficulty": 1,
                    "true_false_flag": True
                }
            }
        ]
        
        for detail in study_details:
            cursor.execute("""
            INSERT INTO study_detail (student_id, original_input_id, details)
            VALUES (%s, %s, %s)
            """, (detail["student_id"], detail["original_input_id"], json.dumps(detail["details"])))
        
        # 4. æ’å…¥å­¦æƒ…æ±‡æ€»æ•°æ®
        summaries = [
            {
                "student_id": 1,
                "grade": 7,
                "from_date": "2024-01-01",
                "to_date": "2024-03-31",
                "subject": "æ•°å­¦",
                "details": {
                    "strength": "ä»£æ•°åŸºç¡€æ‰å®",
                    "weakness": "å‡ ä½•å›¾å½¢è®¡ç®—",
                    "progress": "ä»60åˆ†æé«˜åˆ°75åˆ†",
                    "remarks": "éœ€è¦åŠ å¼ºå‡ ä½•ç»ƒä¹ "
                }
            },
            {
                "student_id": 2,
                "grade": 8,
                "from_date": "2024-01-01",
                "to_date": "2024-03-31",
                "subject": "ç”Ÿç‰©",
                "details": {
                    "strength": "å®éªŒæ“ä½œç†Ÿç»ƒ",
                    "weakness": "ç†è®ºçŸ¥è¯†è®°å¿†",
                    "progress": "ä¿æŒ85åˆ†ä»¥ä¸Š",
                    "remarks": "éœ€è¦åŠ å¼ºçŸ¥è¯†ç‚¹è®°å¿†"
                }
            }
        ]
        
        for summary in summaries:
            cursor.execute("""
            INSERT INTO summary (student_id, grade, from_date, to_date, subject, details)
            VALUES (%s, %s, %s, %s, %s, %s)
            """, (
                summary["student_id"],
                summary["grade"],
                summary["from_date"],
                summary["to_date"],
                summary["subject"],
                json.dumps(summary["details"])
            ))
        
        logger.info("Sample educational data inserted successfully.")

    def get_all_schemas(self) -> List[TableSchema]:
        """Retrieves DDL and descriptions for all tables using standard PostgreSQL queries."""
        try:
            with self._get_connection() as conn:
                with conn.cursor() as cursor:
                    # Get all tables
                    cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_type = 'BASE TABLE'
                    """)
                    tables = [row[0] for row in cursor.fetchall()]
                    
                    schemas = []
                    for table in tables:
                        # ä½¿ç”¨æ ‡å‡†çš„ä¿¡æ¯æ¨¡å¼æŸ¥è¯¢è·å–è¡¨ç»“æ„
                        cursor.execute(f"""
                            SELECT 
                                c.column_name, 
                                c.data_type, 
                                c.is_nullable,
                                c.column_default,
                                pg_catalog.col_description((c.table_schema || '.' || c.table_name)::regclass, c.ordinal_position) AS column_comment
                            FROM information_schema.columns c
                            WHERE c.table_name = '{table}' 
                            AND c.table_schema = 'public'
                            ORDER BY c.ordinal_position
                            """)
                            
                        columns = cursor.fetchall()
                        
                        # æ„å»ºè‡ªå®šä¹‰çš„DDLå­—ç¬¦ä¸²
                        ddl_parts = [f"CREATE TABLE {table} ("]
                        for col in columns:
                            col_def = f"  {col[0]} {col[1]}"
                            if col[2] == 'NO':
                                col_def += " NOT NULL"
                            if col[3]:
                                col_def += f" DEFAULT {col[3]}"
                            if col[4]:  # å¦‚æœæœ‰åˆ—æ³¨é‡Š
                                col_def += f"  -- {col[4]}"
                                print(f"Column {col[0]} comment: {col[4]}")
                            ddl_parts.append(col_def)
                        
                        ddl_parts.append(");")
                        ddl = "\n".join(ddl_parts)
                        # print(f"DDL for table {table}:\n{ddl}\n")   
                        
                        # Add descriptions
                        descriptions = {
                            'students': 'å­¦ç”ŸåŸºæœ¬ä¿¡æ¯è¡¨ï¼ŒåŒ…å«å§“åã€å¹´çº§ã€å‡ºç”Ÿæ—¥æœŸã€æ€§åˆ«ã€åœ°åŒºã€æ•™æç‰ˆæœ¬å’Œå­¦æ ¡ç­‰ä¿¡æ¯ã€‚',
                            'original_input': 'åŸå§‹è¾“å…¥æ•°æ®è¡¨ï¼Œå­˜å‚¨å­¦ç”Ÿçš„åŸå§‹å­¦ä¹ ææ–™ï¼Œå¦‚å›¾ç‰‡ã€æ–‡æ¡£ç­‰ã€‚',
                            'study_detail': 'å­¦ä¹ æ˜ç»†è¡¨(å­˜å‚¨å­¦ç”Ÿé”™é¢˜è®°å½•,åŒ…å«å­—æ®µ:details->>error_typeæ ‡è¯†é”™è¯¯ç±»å‹)',
                            'summary': 'å­¦æƒ…æ±‡æ€»è¡¨ï¼ŒæŒ‰æ—¶é—´æ®µè®°å½•å­¦ç”Ÿçš„å­¦ç§‘ä¼˜åŠ¿å’Œå¼±ç‚¹åˆ†æã€‚'
                        }
                        
                        schemas.append(TableSchema(
                            name=table,
                            ddl=ddl,
                            description=descriptions.get(table, '')
                        ))
                        # print(f'schemas: {schemas}')
                    
                    return schemas
        except Exception as e:
            logger.error(f"Failed to get schemas: {e}", exc_info=True)
            return []

    def execute_sql(self, sql: str) -> QueryResult:
        """Executes a given SQL query and returns the result."""
        logger.info(f"Executing PostgreSQL SQL: {sql.strip()}")
        try:
            with self._get_connection() as conn:
                with conn.cursor(cursor_factory=DictCursor) as cursor:
                    cursor.execute(sql)
                    
                    if cursor.description:  # For SELECT queries
                        rows = cursor.fetchall()
                        data = [dict(row) for row in rows]
                        logger.info(f"SQL executed successfully, returned {len(data)} rows.")
                        return QueryResult(success=True, data=data, sql=sql)
                    else:  # For INSERT/UPDATE/DELETE
                        conn.commit()
                        logger.info("SQL executed successfully (no results returned).")
                        return QueryResult(success=True, data=[], sql=sql)
        except Exception as e:
            logger.error(f"PostgreSQL execution failed: {e}", exc_info=True)
            return QueryResult(success=False, data=[], error=str(e), sql=sql)

class VectorStore:
    """Handles embedding creation and retrieval of relevant schemas."""
    def __init__(self, model_name: str = "text-embedding-v4"):
        try:
            if OpenAI is None:
                raise ImportError("OpenAI package not installed. Please run 'pip install openai'.")
            self.model_name = model_name
            self.client = OpenAI(
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            logger.info(f"VectorStore initialized with DashScope model: {model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize DashScope embedding client: {e}", exc_info=True)
            raise
        self.schemas: List[TableSchema] = []
        self.schema_embeddings: Optional[np.ndarray] = None

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """Get embeddings using DashScope API"""
        try:
            all_embeddings = []
            for text in texts:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=text,
                    dimensions=1024,
                    encoding_format="float"
                )
                print(f"Embedding text: {text}")
                # print(f"Embedding response: {response}")

                all_embeddings.append(response.data[0].embedding)
            return np.array(all_embeddings)
        except Exception as e:
            logger.error(f"Failed to get embeddings: {e}")
            raise

    def build_embeddings(self, schemas: List[TableSchema]):
        """Creates and stores vector embeddings for the given schemas."""
        self.schemas = schemas
        if not self.schemas:
            logger.warning("No schemas provided to build embeddings.")
            return
            
        descriptions = []
        for schema in self.schemas:
            text = f"Table: {schema.name}\nDescription: {schema.description}\nDDL: {schema.ddl}"
            descriptions.append(text)
            logger.info(f"Leo Schema for embedding:\n{text}\n")
        logger.info(f"Creating embeddings for {len(descriptions)} schemas...")
        self.schema_embeddings = self.get_embeddings(descriptions)
        logger.info(f"Built embeddings for {len(self.schemas)} schemas.")

    def retrieve_relevant_schemas(self, question: str, top_k: int = 3) -> List[TableSchema]:
        """Finds the most relevant schemas for a question using cosine similarity."""
        if self.schema_embeddings is None or not self.schemas:
            logger.warning("Embeddings not built. Cannot retrieve schemas.")
            return []
        
        question_embedding = self.get_embeddings([question])
        similarities = cosine_similarity(question_embedding, self.schema_embeddings)[0]
        
        k = min(top_k, len(self.schemas))
        top_indices = np.argsort(similarities)[-k:][::-1]
        
        relevant_schemas = [self.schemas[i] for i in top_indices]
        logger.info(f"Retrieved {len(relevant_schemas)} relevant schemas for the question.")
        for i in top_indices:
            logger.info(f"  - {self.schemas[i].name} (Similarity: {similarities[i]:.4f})")
            
        return relevant_schemas
    
    def multi_path_retrieve_schemas(self, question: str, llm_provider, top_k_per_path: int = 2) -> List[TableSchema]:
        """ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢ç»´åº¦ï¼Œç„¶åå¤šè·¯å¬å›DDLè¡¨ç»“æ„"""
        analysis_prompt = f"""
åˆ†æè¿™ä¸ªæ•™å­¦æŸ¥è¯¢éœ€è¦å“ªäº›æ•°æ®ç»´åº¦ï¼Œè¾“å‡º2-3ä¸ªå…·ä½“çš„æŸ¥è¯¢æ–¹å‘ã€‚
æ¯ä¸ªæ–¹å‘è¦ä½¿ç”¨æœ€ç›´æ¥ã€ç®€æ´çš„å…³é”®è¯ï¼Œä¾¿äºåŒ¹é…æ•°æ®è¡¨åç§°ã€‚

æŸ¥è¯¢: {question}

è¾“å‡ºæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç»´åº¦ï¼š
students
study_detail
summary
original_input
"""
        
        logger.info("LLM analyzing query dimensions...")
        print(f'analysis_prompt: {analysis_prompt}')
        dimensions_text = llm_provider._call_llm(analysis_prompt, "qwen-plus")
        print(f'dimensions_text: {dimensions_text}')
        
        dimensions = [dim.strip() for dim in dimensions_text.split('\n') if dim.strip()]
        logger.info(f"Identified {len(dimensions)} query dimensions: {dimensions}")
        
        all_retrieved_schemas = []
        seen_table_names = set()
        
        for dimension in dimensions:
            logger.info(f"Retrieving dimension: {dimension}")
            
            dimension_embedding = self.get_embeddings([dimension])
            similarities = cosine_similarity(dimension_embedding, self.schema_embeddings)[0]
            
            k = min(top_k_per_path, len(self.schemas)) # Ensure k does not exceed available schemas
            top_indices = np.argsort(similarities)[-k:][::-1] # Top-k indices
            
            for i in top_indices:
                schema = self.schemas[i]
                if schema.name not in seen_table_names:   # Avoid duplicates
                    all_retrieved_schemas.append(schema)
                    seen_table_names.add(schema.name)
                    logger.info(f"  Retrieved {schema.name} (Similarity: {similarities[i]:.4f})")
        
        logger.info(f"Multi-path retrieval completed, retrieved {len(all_retrieved_schemas)} relevant tables")
        return all_retrieved_schemas

class LLMProvider:
    """A wrapper for LLM API calls using OpenAI-compatible interface."""
    def __init__(self, llm_config: Dict[str, Any]):
        self.provider = llm_config.get("provider")
        self.models = llm_config.get("models", {})
        api_key = os.environ.get(llm_config.get("api_key_env", ""))
        
        if not api_key:
            raise ValueError(f"API key not found. Please set the {llm_config.get('api_key_env')} environment variable.")

        if OpenAI is None:
            raise ImportError("OpenAI SDK not installed. Please run 'pip install openai'.")
        
        if self.provider == "dashscope":
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        elif self.provider == "openai":
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
        
        logger.info(f"LLMProvider initialized for '{self.provider}'.")

    def _call_llm(self, prompt: str, model: str) -> str:
        """Internal method to make the actual API call."""
        logger.info(f"Calling LLM ({self.provider}, model: {model})...")
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            print(f"LLM response: {response}")
            return response.choices[0].message.content or ""
        except Exception as e:
            logger.error(f"LLM API call failed: {e}", exc_info=True)
            return f"Error: LLM call failed. {e}"

    def generate_sql(self, prompt: str) -> str:
        """Generates SQL from a prompt."""
        model = self.models.get("sql_generation", "qwen-plus")
        print(f'Generating SQL with model: {model}')
        # print(f'Generating SQL Prompt: {prompt}')
        sql = self._call_llm(prompt, model)
        print(f'Raw Generated SQL: {sql}')
        return sql.replace("```sql", "").replace("```", "").strip()
    
    def generate_answer(self, prompt: str) -> str:
        """Generates a natural language answer from a prompt."""
        # print(f'Generating Answer Prompt: {prompt}')
        model = self.models.get("answer_generation", "qwen-plus")
        print(f'Generating Answer with model: {model}')
        return self._call_llm(prompt, model).strip()

class NL2SQLPipeline:
    """Orchestrates the Text-to-SQL process for educational data analysis."""
    def __init__(self, config: Dict[str, Any]):
        logger.info("Initializing NL2SQL Pipeline for PostgreSQL...")
        self.db_manager = PGManager(config['database'])
        self.vector_store = VectorStore(config['embedding_model'])
        self.llm_provider = LLMProvider(llm_config=config['llm'])
        
        all_schemas = self.db_manager.get_all_schemas()
        logger.info(f"Creating embeddings for {len(all_schemas)} schemas...")
        self.vector_store.build_embeddings(all_schemas)
        
        self.sql_prompt_template = config['prompts']['sql_generation']
        self.answer_prompt_template = config['prompts']['answer_generation']
        logger.info("NL2SQL Pipeline initialized successfully.")

    def ask(self, question: str) -> Dict[str, Any]:
        """Executes the full Text-to-SQL pipeline for a given question."""
        import time
        start_time = time.time()
        
        logger.info("=" * 80)
        logger.info(f"Processing question: {question}")
        logger.info("=" * 80)

        # 1. Retrieve relevant schemas
        logger.info("Step 1: Starting multi-path vector retrieval...")
        retrieval_start = time.time()
        relevant_schemas = self.vector_store.multi_path_retrieve_schemas(question, self.llm_provider)
        retrieval_time = time.time() - retrieval_start
        
        schema_context = "\n\n".join([f"--- Table: {s.name} ---\n{s.ddl}" for s in relevant_schemas])

        # 2. Generate SQL
        logger.info("Step 2: Starting SQL generation...")
        sql_start = time.time()
        sql_prompt = self.sql_prompt_template.format(schema_context=schema_context, question=question)
        print(f'sql_prompt: {sql_prompt}')
        sql_query = self.llm_provider.generate_sql(sql_prompt)
        sql_time = time.time() - sql_start
        
        if sql_query.strip().startswith("SCHEMA_INSUFFICIENT:"):
            logger.warning("LLM rejected SQL generation due to insufficient DDL")
            
            total_time = time.time() - start_time
            return {
                'question': question,
                'relevant_schemas': [s.name for s in relevant_schemas],
                'sql_query': None,
                'data': [],
                'answer': f"å½“å‰æ•°æ®åº“ç»“æ„æ— æ³•æ»¡è¶³æŸ¥è¯¢éœ€æ±‚ã€‚{sql_query.replace('SCHEMA_INSUFFICIENT:', '').strip()}",
                'query_success': False,
                'schema_insufficient': True,
                'performance': {
                    'retrieval_time': retrieval_time,
                    'sql_generation_time': sql_time,
                    'execution_time': 0,
                    'answer_generation_time': 0,
                    'total_time': total_time
                }
            }
        
        logger.info(f"Generated SQL: {sql_query}")

        # 3. Execute SQL
        logger.info("Step 3: Starting database query execution...")
        exec_start = time.time()
        query_result = self.db_manager.execute_sql(sql_query)
        exec_time = time.time() - exec_start
        
        # 4. Generate Answer
        logger.info("Step 4: Starting natural language answer generation...")
        answer_start = time.time()
        answer = ""
        
        if query_result.success:
            if not query_result.data:
                answer = "æŸ¥è¯¢æˆåŠŸä½†æ²¡æœ‰åŒ¹é…çš„æ•°æ®ã€‚"
            else:
                data_summary = self._create_data_summary(query_result.data)
                answer_prompt = self.answer_prompt_template.format(
                    question=question,
                    sql_query=sql_query,
                    data_summary=data_summary
                )
                print(f'answer_prompt: {answer_prompt}')
                answer = self.llm_provider.generate_answer(answer_prompt)
        else:
            answer = f"æŸ¥è¯¢æ‰§è¡Œå‡ºé”™: {query_result.error}"
        
        answer_time = time.time() - answer_start
        total_time = time.time() - start_time
        
        return {
            "question": question,
            "relevant_schemas": [s.name for s in relevant_schemas],
            "sql_query": query_result.sql,
            "query_success": query_result.success,
            "query_error": query_result.error,
            "data": query_result.data,
            "answer": answer,
            "performance": {
                "retrieval_time": retrieval_time,
                "sql_generation_time": sql_time,
                "execution_time": exec_time,
                "answer_generation_time": answer_time,
                "total_time": total_time
            }
        }

    def _create_data_summary(self, data: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºæ•°æ®æ‘˜è¦ï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯"""
        if not data:
            return "æ²¡æœ‰å¯ç”¨æ•°æ®ã€‚"

        all_columns = set()
        column_types = {}
        
        for record in data:
            all_columns.update(record.keys())
            for key, value in record.items():
                if key not in column_types:
                    column_types[key] = type(value).__name__

        summary_data = {
            "total_records": len(data),
            "columns_info": {
                col: column_types.get(col, "unknown") 
                for col in sorted(list(all_columns))
            },
            "data_structure": "æ•™å­¦æ•°æ®åˆ†æç»“æœ",
            "privacy_note": "å®é™…æ•°æ®å€¼å·²çœç•¥"
        }
        
        return json.dumps(summary_data, indent=2, ensure_ascii=False)

# --- Demo Execution ---------------------------------------------------------
def run_demo():
    """Sets up the pipeline and runs a demo with educational questions."""
    print("=" * 60)
    print("ğŸ“š æ•™è‚²æ•°æ®åˆ†æ NL2SQL æ¼”ç¤º (PostgreSQL)")
    print("=" * 60)

    try:
        api_key_env = CONFIG['llm']['api_key_env']
        if not os.environ.get(api_key_env):
            print(f"\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ '{api_key_env}' æœªè®¾ç½®ã€‚")
            print("è¯·è®¾ç½®æ‚¨çš„APIå¯†é’¥ä»¥ç»§ç»­ã€‚")
            return
    
        pipeline = NL2SQLPipeline(CONFIG)
        
        demo_questions = [
            # {
            #     "question": "è¯·æŸ¥æ‰¾å¼ ä¸‰çš„8æœˆçš„å­¦æƒ…æ±‡æ€»ï¼Œåªè¿”å›detailså­—æ®µï¼Ÿ",
            #     "description": "æŸ¥è¯¢å­¦ç”Ÿå­¦æƒ…æ±‡æ€»"
            # }
            {
                "question": "è¯·æŸ¥æ‰¾å¼ ä¸‰çš„é”™é¢˜è¯¦æƒ…ï¼Ÿ",
                "description": "æŸ¥è¯¢å­¦ç”Ÿé”™é¢˜æ˜ç»†"
            }
            # {
            #     "question": "è¯·æŸ¥æ‰¾å¼ ä¸‰çš„å­¦ä¹ æ˜ç»†ï¼Ÿ",
            #     "description": "æŸ¥è¯¢å­¦ç”Ÿå­¦ä¹ æ˜ç»†"
            # }
            # {
            #     "question": "å¼ ä¸‰åœ¨å­¦æƒ…æ±‡æ€»è¡¨æœ‰å¤šå°‘æ¡è®°å½•ï¼Ÿ",
            #     "description": "æŸ¥è¯¢å­¦ç”Ÿå­¦æƒ…æ±‡æ€»æ•°é‡"
            # },
            # {
            #     "question": "å­¦ä¹ æ˜ç»†è¡¨é‡Œæœ‰å¤šå°‘æ¡è®°å½•ï¼Ÿ",
            #     "description": "æŸ¥è¯¢å­¦ç”Ÿé”™é¢˜æ•°é‡"
            # }
            # {
            #     "question": "æ¯”è¾ƒä¸åŒåœ°åŒº(åä¸œã€ååŒ—ã€åå—)å­¦ç”Ÿä½¿ç”¨ä¸åŒæ•™æç‰ˆæœ¬çš„å­¦ä¹ æ•ˆæœå·®å¼‚",
            #     "description": "ğŸŒ åœ°åŒºæ•™ææ•ˆæœå¯¹æ¯”"
            # },
            # {
            #     "question": "æ‰¾å‡ºåœ¨å‡ ä½•çŸ¥è¯†ç‚¹ä¸Šè¡¨ç°æœ€å·®çš„å­¦ç”Ÿï¼Œå¹¶åˆ†æä»–ä»¬çš„é”™é¢˜æ¨¡å¼",
            #     "description": "ğŸ” ç‰¹å®šçŸ¥è¯†ç‚¹å­¦ç”Ÿåˆ†æ"
            # }
        ]
        
        print("\næ•™è‚²æ•°æ®åˆ†ææ¼”ç¤ºå¼€å§‹ - å±•ç¤ºå¤æ‚æ•™å­¦åœºæ™¯çš„å¤šè¡¨æŸ¥è¯¢åˆ†æ")
        print("æ•°æ®åº“è§„æ¨¡: 4ä¸ªæ ¸å¿ƒæ•™å­¦æ•°æ®è¡¨")
        print("=" * 90)
        
        import time
        total_start_time = time.time()
        demo_stats = {
            "total_questions": len(demo_questions),
            "successful_queries": 0,
            "failed_queries": 0,
            "total_tables_used": set(),
            "total_execution_time": 0,
            "performance_breakdown": [],
            "schema_insufficient_queries": 0
        }
        
        for i, demo in enumerate(demo_questions, 1):
            question = demo["question"]
            description = demo["description"]
            
            print(f"\n[åˆ†æä»»åŠ¡ {i}/{len(demo_questions)}] {description}")
            print(f"é—®é¢˜: {question}")
            print("å¤„ç†ä¸­...")
            
            demo_start_time = time.time()
            result = pipeline.ask(question)
            demo_time = time.time() - demo_start_time
            
            demo_stats["total_execution_time"] += demo_time
            demo_stats["total_tables_used"].update(result['relevant_schemas'])
            
            if result['query_success']:
                demo_stats["successful_queries"] += 1
            elif result.get('schema_insufficient', False):
                demo_stats["failed_queries"] += 1
                demo_stats["schema_insufficient_queries"] += 1
            else:
                demo_stats["failed_queries"] += 1
            
            demo_stat = {
                "demo_number": i,
                "description": description,
                "success": result['query_success'],
                "schema_insufficient": result.get('schema_insufficient', False),
                "tables_used": len(result['relevant_schemas']),
                "table_names": result['relevant_schemas'],
                "data_records": len(result['data']) if result['query_success'] else 0,
                "execution_time": demo_time,
                "performance": result.get('performance', {})
            }
            demo_stats["performance_breakdown"].append(demo_stat)
            
            print("-" * 80)
            print("åˆ†æç»“æœ:")
            print("-" * 80)
            print(f"AIè¯†åˆ«çš„ç›¸å…³è¡¨: {', '.join(result['relevant_schemas'])}")
            print(f"\nç”Ÿæˆçš„SQLæŸ¥è¯¢:")
            print(f"```sql")
            print(result['sql_query'])
            print(f"```")
            
            if result['query_success']:
                print(f"\næŸ¥è¯¢æ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(result['data'])} æ¡ç»“æœ")
                print(f"\nAIåˆ†æ:")
                print("=" * 50)
                print(result['answer'])
                print("=" * 50)
                
                if result['data']:
                    print(f"\nå…³é”®æ•°æ®æ‘˜è¦ ({len(result['data'])} æ¡è®°å½•):")
                    for idx, record in enumerate(result['data'][:3], 1):
                        print(f"  {idx}. {record}")
                    if len(result['data']) > 3:
                        print(f"  ... å’Œ {len(result['data']) - 3} æ¡æ›´å¤šè®°å½•")
            else:
                print(f"\næŸ¥è¯¢æ‰§è¡Œé‡åˆ°é—®é¢˜: {result.get('query_error', result.get('answer', 'æœªçŸ¥é”™è¯¯'))}")
            
            print("=" * 90)
            if i < len(demo_questions):
                print("å‡†å¤‡ä¸‹ä¸€ä¸ªåˆ†æä»»åŠ¡... (2ç§’)")
                time.sleep(2)
        
        total_demo_time = time.time() - total_start_time
        
        print("\n" + "=" * 60)
        print("æ•™è‚²æ•°æ®åˆ†ææ¼”ç¤ºå®Œæˆ!")
        print("=" * 60)
        
        print("\nAIå±•ç¤ºäº†å¼ºå¤§çš„æ•™å­¦æ•°æ®åˆ†æèƒ½åŠ›:")
        print("   âœ“ æ™ºèƒ½å…³è”4ä¸ªæ ¸å¿ƒæ•™å­¦æ•°æ®è¡¨")
        print("   âœ“ è‡ªåŠ¨ç”Ÿæˆå¤æ‚æ•™å­¦åˆ†æçš„SQLæŸ¥è¯¢")
        print("   âœ“ å¤šç»´åº¦æ•°æ®èšåˆå’Œæ·±åº¦æ´å¯Ÿ")
        
        print(f"\næŠ€æœ¯ç»Ÿè®¡:")
        print(f"   åˆ†æä»»åŠ¡æ•°é‡: {demo_stats['total_questions']} ä¸ªæ•™å­¦åœºæ™¯")
        print(f"   æŸ¥è¯¢æˆåŠŸç‡: {demo_stats['successful_queries']}/{demo_stats['total_questions']} ({demo_stats['successful_queries']/demo_stats['total_questions']*100:.1f}%)")
        print(f"   æ¶‰åŠæ•°æ®è¡¨: {len(demo_stats['total_tables_used'])} ä¸ªæ•™å­¦æ ¸å¿ƒè¡¨")
        print(f"   å¹³å‡è¡¨å…³è”æ•°: {sum(len(stat['table_names']) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.1f} è¡¨/æŸ¥è¯¢")
        print(f"   æ€»æ—¶é—´: {total_demo_time:.1f}ç§’ (å¹³å‡ {total_demo_time/demo_stats['total_questions']:.1f}ç§’/é—®é¢˜)")
        
        print("\nè¿™æ˜¯æ–°ä¸€ä»£æ•™è‚²æ•°æ®åˆ†æNL2SQLç³»ç»Ÿçš„çœŸå®èƒ½åŠ›!")

    except (ValueError, ImportError) as e:
        print(f"\nâŒ è®¾ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    except Exception as e:
        logger.error("An unexpected error occurred during the demo.", exc_info=True)
        print(f"\nâŒ å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

if __name__ == "__main__":
    run_demo()